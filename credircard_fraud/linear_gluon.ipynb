{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "import pandas as pd\n",
    "\n",
    "from sagemaker.mxnet import MXNet\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::475933981307:role/service-role/AmazonSageMaker-ExecutionRole-20180102T172706'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {'train_data': 'data/train_data.npy', \n",
    "             'train_label': 'data/train_label.npy',\n",
    "             'val_data': 'data/val_data.npy',\n",
    "             'val_label': 'data/val_label.npy'\n",
    "            }\n",
    "\n",
    "hyper_parameters = {'batch_size': 100,\n",
    "                   'learning_rate': 0.01,\n",
    "                   'num_epochs': 1,\n",
    "                   'optimizer': 'sgd',\n",
    "                   'momentum': 0.9,\n",
    "                   'num_dims': 30}\n",
    "\n",
    "train_ctx = mx.cpu()\n",
    "num_examples = 10000\n",
    "num_input = 30\n",
    "num_outputs = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "\r\n",
      "import logging\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, autograd,nd\r\n",
      "from mxnet.gluon import nn\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import time\r\n",
      "from mxnet.gluon.data.vision import MNIST\r\n",
      "import boto3\r\n",
      "import gzip,struct\r\n",
      "\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training methods                                             #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "\r\n",
      "def train(channel_input_dirs, hyperparameters, hosts, num_gpus, **kwargs):\r\n",
      "    # SageMaker passes num_cpus, num_gpus and other args we can use to tailor training to\r\n",
      "    # the current container environment, but here we just use simple cpu context.\r\n",
      "    ctx = mx.gpu() if num_gpus > 0 else mx.cpu()\r\n",
      "\r\n",
      "    # retrieve the hyperparameters we set in notebook (with some defaults)\r\n",
      "    batch_size = hyperparameters.get('batch_size', 100)\r\n",
      "    epochs = hyperparameters.get('num_epochs', 10)\r\n",
      "    learning_rate = hyperparameters.get('learning_rate', 0.1)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    log_interval = hyperparameters.get('log_interval', 100)\r\n",
      "    num_dims = hyperparameter.get('num_dims')\r\n",
      "    num_output = hyperparameter.get('num_outputs', 2)\r\n",
      "    \r\n",
      "\r\n",
      "    # load training and validation data\r\n",
      "    (train_data_iter, test_data_iter) = load_data(filenames)\r\n",
      "\r\n",
      "    # define the network\r\n",
      "    net = define_network()\r\n",
      "\r\n",
      "    # Collect all parameters from net and its children, then initialize them.\r\n",
      "    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\r\n",
      "    # Trainer is for updating parameters with gradient.\r\n",
      "\r\n",
      "    if len(hosts) == 1:\r\n",
      "        kvstore = 'device' if num_gpus > 0 else 'local'\r\n",
      "    else:\r\n",
      "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\r\n",
      "\r\n",
      "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\r\n",
      "                            {'learning_rate': learning_rate, 'momentum': momentum},\r\n",
      "                            kvstore=kvstore)\r\n",
      "    metric = mx.metric.RMSE()\r\n",
      "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\r\n",
      "\r\n",
      "    for epoch in range(epochs):\r\n",
      "        # reset data iterator and metric at begining of epoch.\r\n",
      "        metric.reset()\r\n",
      "        btic = time.time()\r\n",
      "        for i, (data, label) in enumerate(train_data):\r\n",
      "            # Copy data to ctx if necessary\r\n",
      "            data = data.as_in_context(ctx)\r\n",
      "            label = label.as_in_context(ctx)\r\n",
      "            # Start recording computation graph with record() section.\r\n",
      "            # Recorded graphs can then be differentiated with backward.\r\n",
      "            with autograd.record():\r\n",
      "                output = net(data)\r\n",
      "                L = loss(output, label)\r\n",
      "                L.backward()\r\n",
      "            # take a gradient step with batch_size equal to data.shape[0]\r\n",
      "            trainer.step(data.shape[0])\r\n",
      "            # update metric at last.\r\n",
      "            metric.update([label], [output])\r\n",
      "\r\n",
      "            if i % log_interval == 0 and i > 0:\r\n",
      "                name, acc = metric.get()\r\n",
      "                print('[Epoch %d Batch %d] Training: %s=%f, %f samples/s' %\r\n",
      "                      (epoch, i, name, acc, batch_size / (time.time() - btic)))\r\n",
      "\r\n",
      "            btic = time.time()\r\n",
      "\r\n",
      "        name, acc = metric.get()\r\n",
      "        print('[Epoch %d] Training: %s=%f' % (epoch, name, acc))\r\n",
      "\r\n",
      "        name, val_acc = evaluate_accuracy(net, test_data_iter, ctx)\r\n",
      "        print('[Epoch %d] Validation: %s=%f' % (epoch, name, val_acc))\r\n",
      "\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def save(net, model_dir):\r\n",
      "    # save the model\r\n",
      "    y = net(mx.sym.var('data'))\r\n",
      "    y.save('%s/model.json' % model_dir)\r\n",
      "    net.collect_params().save('%s/model.params' % model_dir)\r\n",
      "\r\n",
      "def load_data(data_files):\r\n",
      "    train_data = np.load(data_files['train_data']).astype(np.float32)\r\n",
      "    train_label = np.load(data_files['train_label']).astype(np.float32)\r\n",
      "    val_data = np.load(data_files['val_data']).astype(np.float32)\r\n",
      "    val_label = np.load(data_files['val_label']).astype(np.float32)\r\n",
      "    return (train_data, train_label), (val_data, val_label)\r\n",
      "    \r\n",
      "def define_network(num_inputs, num_outputs):\r\n",
      "    net = nn.Sequential()\r\n",
      "    with net.name_scope():\r\n",
      "        net.add(gluon.nn.Dense(num_outputs, in_units=num_input))\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def get_train_data(data,batch_size):\r\n",
      "    return gluon.data.DataLoader(gluon.data.ArrayDataset(data[0], data[1]), \r\n",
      "                                        shuffle=True, batch_size=batch_size)\r\n",
      "\r\n",
      "\r\n",
      "def get_val_data(data_dir,batch_size):\r\n",
      "    return gluon.data.DataLoader(\r\n",
      "        gluon.data.vision.MNIST(data_dir, train=False, transform=input_transformer),\r\n",
      "        batch_size=batch_size, shuffle=False)\r\n",
      "\r\n",
      "  \r\n",
      "def evaluate_accuracy(net, data_iterator, ctx):\r\n",
      "    metric = mx.metric.RMSE()\r\n",
      "    for i, (data, label) in enumerate(data_iterator):\r\n",
      "        data = data.as_in_context(ctx)\r\n",
      "        label = label.as_in_context(ctx)\r\n",
      "        output = net(data)\r\n",
      "        predictions = nd.argmax(output, axis=1)\r\n",
      "        metric.update(preds=predictions, labels=label)\r\n",
      "    return metric.get()\r\n",
      "\r\n",
      "        \r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Hosting methods                                              #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    Load the gluon model. Called once when hosting service starts.\r\n",
      "\r\n",
      "    :param: model_dir The directory where model files are stored.\r\n",
      "    :return: a model (in this case a Gluon network)\r\n",
      "    \"\"\"\r\n",
      "    symbol = mx.sym.load('%s/model.json' % model_dir)\r\n",
      "    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\r\n",
      "    inputs = mx.sym.var('data')\r\n",
      "    param_dict = gluon.ParameterDict('model_')\r\n",
      "    net = gluon.SymbolBlock(outputs, inputs, param_dict)\r\n",
      "    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    \"\"\"\r\n",
      "    Transform a request using the Gluon model. Called once per request.\r\n",
      "\r\n",
      "    :param net: The Gluon model.\r\n",
      "    :param data: The request payload.\r\n",
      "    :param input_content_type: The request content type.\r\n",
      "    :param output_content_type: The (desired) response content type.\r\n",
      "    :return: response payload and content type.\r\n",
      "    \"\"\"\r\n",
      "    # we can use content types to vary input/output handling, but\r\n",
      "    # here we just assume json for both\r\n",
      "    parsed = json.loads(data)\r\n",
      "    nda = mx.nd.array(parsed)\r\n",
      "    output = net(nda)\r\n",
      "    prediction = mx.nd.argmax(output, axis=1)\r\n",
      "    response_body = json.dumps(prediction.asnumpy().tolist()[0])\r\n",
      "    return response_body, output_content_type"
     ]
    }
   ],
   "source": [
    "!cat linear_model_gluon.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MXNet('linear_model_gluon.py', \n",
    "          role=role,\n",
    "          train_instance_count=1,\n",
    "          train_instance_type=\"ml.c4.xlarge\",\n",
    "          hyperparameters=hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-146a262add4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "m.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_files):\n",
    "    train_data = np.load(data_files['train_data']).astype(np.float32)\n",
    "    train_label = np.load(data_files['train_label']).astype(np.float32)\n",
    "    val_data = np.load(data_files['val_data']).astype(np.float32)\n",
    "    val_label = np.load(data_files['val_label']).astype(np.float32)\n",
    "    return (train_data, train_label), (val_data, val_label)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_data(data,batch_size):\n",
    "    return gluon.data.DataLoader(gluon.data.ArrayDataset(data[0], data[1]), \n",
    "                                        shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_eval_data(data, batch_size):\n",
    "    return gluon.data.DataLoader(gluon.data.ArrayDataset(data[0], data[1]), \n",
    "                                        shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data = np.load('data/train_data.npy').astype(np.float32)\n",
    "train_label = np.load('data/train_label.npy').astype(np.float32)\n",
    "\n",
    "val_data = np.load('data/val_data.npy').astype(np.float32)\n",
    "val_label = np.load('data/val_label.npy').astype(np.float32)\n",
    "\n",
    "print(\"training data shape= {}; training label shape = {} \\nValidation data shape= {}; validation label shape = {}\".format(train_data.shape, \n",
    "                                                                        train_label.shape,\n",
    "                                                                        val_data.shape,\n",
    "                                                                        val_label.shape))\n",
    "train_set = (train_data, train_label)\n",
    "test_set = (val_data, val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = load_data(data_files)\n",
    "\n",
    "\n",
    "\n",
    "train_data_iter = get_train_data(train_set, batch_size)\n",
    "\n",
    "\n",
    "test_data_iter = get_eval_data(test_set, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170884, 30) (170884,) (113923, 30) (113923,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0].shape, train_set[1].shape, test_set[0].shape, test_set[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_outputs, in_units=num_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential2_ (\n",
       "  Parameter sequential2_dense0_weight (shape=(2, 30), dtype=<class 'numpy.float32'>)\n",
       "  Parameter sequential2_dense0_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=train_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(params=net.collect_params(), \n",
    "                            optimizer=hyper_parameters.get('optimizer', 'sgd'), \n",
    "                            optimizer_params={'learning_rate':hyper_parameters.get('learning_rate', 0.01)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def evaluate_accuracy(net, data_iterator, ctx):\n",
    "    metric = mx.metric.RMSE()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        metric.update(preds=predictions, labels=label)\n",
    "    return metric.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = evaluate_accuracy(net, test_data_iter, train_ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rmse', 0.0016063481474329151)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 25136.616999, Train_acc 0.00182579995787, Test_acc 0.00160634814743\n"
     ]
    }
   ],
   "source": [
    "batch_size = hyper_parameters.get('batch_size', 100)\n",
    "loss_sequence = []\n",
    "num_batches = num_examples / batch_size\n",
    "\n",
    "for e in range(hyper_parameters.get('num_epochs', 10)):\n",
    "    cumulative_loss = 0\n",
    "    # inner loop\n",
    "    for i, (data, label) in enumerate(train_data_iter):\n",
    "        data = data.as_in_context(train_ctx)\n",
    "        label = label.as_in_context(train_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        cumulative_loss += nd.mean(loss).asscalar()\n",
    "    \n",
    "    net.save_params('model/l_r_'+str(e)+'.params')\n",
    "    \n",
    "    test_accuracy = evaluate_accuracy(test_data_iter, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data_iter, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train_data,batch_size):\n",
    "    return gluon.data.DataLoader(gluon.data.ArrayDataset(train_set[0], train_set[1]), \n",
    "                                        shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
